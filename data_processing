# -*- coding:utf-8 -*-

import os
import math
import scipy.io as scio
import pickle
import numpy as np
import scipy
import scipy.stats as scit
import matplotlib.pyplot as plt
from scipy.signal import medfilt, filtfilt, butter
import pandas as pd
from matplotlib.backends.backend_pdf import PdfPages
from imblearn.combine import SMOTEENN as smote

Z_Score = True  # 对训练数据是否进行zscore归一化
Medfilt = True  # 对训练数据进行中值滤波
SAVE_FIG = False  # 是否进行数据可视化
SMOTEENN = False  # 对数据集进行过采样和欠采样
train_percentage = 0.7  # 训练数据比例
features = 12  # 特征数目，采用十二导联所以是12
path = './data/'  # 数据路径
pickle_save = './processing_data/'  # 经过处理后保存数据路径

if not os.path.exists(pickle_save):
    os.mkdir(pickle_save)


def baseline_correction(signals):
    base_line = medfilt(signals, 99)
    base_line = medfilt(base_line, 299)
    signals = signals - base_line
    return signals


def ecg_preprocess(record, c):
    for i in range(c):
        record[:, i] = baseline_correction(record[:, i])
    return record


def smote_data(data, label):
    # h = label.shape
    smote_enn = smote(random_state=666)
    labels = []
    for i in range(label.shape[0]):
        if np.all([label[i,] == [0., 0., 1.]]):
            labels.append(0)
        elif np.all([label[i,] == [1., 0., 0.]]):
            labels.append(1)
        elif np.all([label[i,] == [0., 1., 0.]]):
            labels.append(2)
        elif np.all([label[i,] == [1., 1., 0.]]):
            labels.append(3)
    labels = np.array(labels)
    data_resampled, label_resampled = smote_enn.fit_resample(data, labels)
    labels_resampled = np.zeros((0, 3))
    for i in range(label_resampled.shape[0]):
        if label_resampled[i] == 0:
            labels_resampled = np.vstack((labels_resampled, [0, 0, 1]))
        elif label_resampled[i] == 1:
            labels_resampled = np.vstack((labels_resampled, [1, 0, 0]))
        elif label_resampled[i] == 2:
            labels_resampled = np.vstack((labels_resampled, [0, 1, 0]))
        elif label_resampled[i] == 3:
            labels_resampled = np.vstack((labels_resampled, [1, 1, 0]))
    return data_resampled, labels_resampled


def read_data(path):
    # 获得文件名称
    dataname = os.listdir(path + 'Train/')
    filesname = [path + 'Train/' + x for x in dataname if x[-4:] == '.mat']
    # 提取labels标签，并分类
    labels_name = path + 'Train.xlsx'
    labels_pd = pd.read_excel(labels_name)
    (h, v) = labels_pd.shape
    labels = np.zeros((1, 3))
    for i in range(h):
        if labels_pd['Others'][i] == 1:
            labels = np.vstack((labels, [0, 0, 1]))
        else:
            if labels_pd['STE'][i] == 1:
                if labels_pd['STD'][i] == 1:
                    labels = np.vstack((labels, [1, 1, 0]))
                else:
                    labels = np.vstack((labels, [1, 0, 0]))
            elif labels_pd['STE'][i] == 0 and labels_pd['STD'][i] == 1:
                labels = np.vstack((labels, [0, 1, 0]))
    labels = labels[1:]
    # 提取数据值
    files = np.zeros((1, 7500, features))
    for i, file in enumerate(filesname):
        # # 仅调试用
        # if i == 400:
        #     break

        data = scio.loadmat(file)
        data = data['ecg'][:features, ].T.reshape(1, 7500, features)
        files = np.vstack((files, data))

    # # 仅调试用
    # labels = labels[:400]

    # 去掉数据值的初始化实例
    files = files[1:]
    return files, labels


def random_dataset(data, labels):
    p = np.random.permutation(range(len(labels)))
    random_data, random_labels = data[p], labels[p]
    return random_data, random_labels


def main():
    # 提取数据集，并划分为数据和标签
    ori_data, labels = read_data(path)

    # 对信号进行滤波
    if Medfilt:
        print("----------------start Medflit----------------")
        # # 平滑滤波
        for i in range(ori_data.shape[0]):
            for j in range(ori_data.shape[2]):
                b, a = butter(5, [0.0008, 0.4], 'bandpass')
                ori_data[i, :, j] = filtfilt(b, a, ori_data[i, :, j]
                                             .reshape(ori_data.shape[1]))
                ori_data[i, :, j] = scipy.signal.savgol_filter(ori_data[i, :, j]
                                                               .reshape(ori_data.shape[1]), 5, 2)
        # 中值滤波
        medfilt_data = np.zeros((1, 7500, features))
        for i in range(ori_data.shape[0]):
            signal_data = ecg_preprocess(ori_data[i, :, :], ori_data.shape[2]).reshape(1, 7500, features)
            medfilt_data = np.vstack((medfilt_data, signal_data))
        medfilt_data = medfilt_data[1:]
        data = medfilt_data
    else:
        data = ori_data

    # 使用SMOTEENN算法提高数据集中数据的平衡
    if SMOTEENN:
        print("----------------start SMOTE----------------")
        (h, v, c) = data.shape
        data = data.reshape((h, v * c))
        oversampling_data, oversampling_labels = smote_data(data, labels)
        oversampling_data = oversampling_data.reshape((oversampling_data.shape[0], v, c))
        oversampling_data = np.vstack((oversampling_data, data.reshape(h, v, c)))
        oversampling_labels = np.vstack((oversampling_labels, labels))
        # for i in range(1, ori_data.shape[2]):
        #     oversampling_data_one, _ = smote_data(ori_data[:, :, i].reshape(h, v), labels)
        #     oversampling_data = np.dstack((oversampling_data,
        #                                   oversampling_data_one.reshape(oversampling_data_one.shape[0],
        #                                                                 oversampling_data_one.shape[1],
        #                                                                 1)[:oversampling_data.shape[0]]))
        labels = oversampling_labels
        data = oversampling_data

    # 对信号进行zscore归一化处理
    if Z_Score:
        print("----------------start Z_Score----------------")
        for i in range(data.shape[0]):
            for j in range(data.shape[2]):
                data[i, :, j] = scit.zscore(data[i, :, j])

    # 保证标签与数据通道相等
    assert labels.shape[0] == data.shape[0]

    # #仅调试用(快速可视化)
    # plt.figure(figsize=(12, 10))
    # x = [10 * i for i in range(500)]
    # plt.plot(x, data[0, :500, 0], color='blue')
    # plt.plot(x, data[0, :500, 1], color='red')
    # plt.plot(x, data[0, :500, 2], color='green')
    # plt.show()

    # 保存数据图片(pdf保存处理后的可视化数据)
    if SAVE_FIG:
        print("----------------start save_fig----------------")
        with PdfPages(pickle_save + 'example.pdf') as pdf:
            Seqlength = 3000
            # for i in range(data.shape[0]):
            for i in range(50):
                signal = data[i, :Seqlength, 0]
                x = np.arange(Seqlength)
                plt.plot(x, signal)
                pdf.savefig()
                plt.close()

    # 将数据集片段打乱
    random_data, random_labels = random_dataset(data, labels)

    # 划分训练集和验证集，然后保存
    nums = len(random_data)
    train_len = int(math.ceil(nums * train_percentage))
    data_train, labels_train = random_data[:train_len], random_labels[:train_len]
    data_val, labels_val = random_data[train_len:], random_labels[train_len:]

    # 以pkl形式保存，以保证读取速度和处理效率
    print("----------------start save data----------------")
    with open(pickle_save + 'train_data.pkl', 'wb') as f1:
        pickle.dump((data_train, labels_train), f1)

    with open(pickle_save + 'val_data.pkl', 'wb') as f2:
        pickle.dump((data_val, labels_val), f2)


if __name__ == '__main__':
    main()
